# Adversarial examples in the physical world

## 一、概述

### AE(Adversarial Examples)的存在对于许多ML(Machine Learning)分类器是一个巨大的威胁，它们往往能用小到人眼无法察觉的扰动来使分类器得出的结果南辕北辙。但是之前的工作都是基于直接在样本上改动，如果在现实生活中进行变换，结果会如何呢？本文采用ImageNet作为攻击对象，发现即使在现实生活中，变换后的AE仍能对其造成不小的威胁。

## 二、简介

### ML(Machine Learning)分类器在很多领域有着出类拔萃的效果，这是一个不争的事实；但是其对于AE的脆弱性同样不容忽视。这种AE所加的扰动往往人眼不可察，但是却能使得ML分类器屡屡犯错。同时，AE还对不同ML和不同训练集具有普适性，这就意味着对于ML分类器的黑箱攻击是可能的。
### 然而之前的AE生成模型都是直接在数据上操作，而缺少在现实生活中AE的实例。之前的工作要不就是没有在现实生活中进行，要不就是扰动太大不足以称为AE。(当然和它相近的有一篇Sharif,2016的论文，比这篇论文更为精细)
### 本文采用的是白箱攻击(ML分类器的参数已知)，直接用相机拍摄生成AE所打印的照片作为分类器输入，之后也会对这种变换做一定的理论解释。
